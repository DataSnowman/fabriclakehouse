{"cells":[{"cell_type":"code","execution_count":1,"id":"5b8ac155-4305-44e0-bee5-69656cdbd6cd","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[{"data":{"application/vnd.livy.statement-meta+json":{"execution_finish_time":"2023-09-13T15:48:54.6386952Z","execution_start_time":"2023-09-13T15:48:52.4033724Z","livy_statement_state":"available","parent_msg_id":"cffba9e7-f2fa-43cf-a1c9-2685857e6ce1","queued_time":"2023-09-13T15:46:00.1325105Z","session_id":"f423086e-808a-4b6a-ae13-87c4e1daa3ae","session_start_time":"2023-09-13T15:46:00.848683Z","spark_jobs":{"jobs":[],"limit":20,"numbers":{"FAILED":0,"RUNNING":0,"SUCCEEDED":0,"UNKNOWN":0},"rule":"ALL_DESC"},"spark_pool":null,"state":"finished","statement_id":3},"text/plain":["StatementMeta(, f423086e-808a-4b6a-ae13-87c4e1daa3ae, 3, Finished, Available)"]},"metadata":{},"output_type":"display_data"}],"source":["from pyspark.sql.types import *\n","import json\n","\n","jschemacsv = '{\"fields\":[{\"metadata\":{},\"name\":\"Prscrbr_NPI\",\"nullable\":true,\"type\":\"integer\"},{\"metadata\":{},\"name\":\"Prscrbr_Last_Org_Name\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"Prscrbr_First_Name\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"Prscrbr_City\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"Prscrbr_State_Abrvtn\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"Prscrbr_State_FIPS\",\"nullable\":true,\"type\":\"integer\"},{\"metadata\":{},\"name\":\"Prscrbr_Type\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"Prscrbr_Type_Src\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"Brnd_Name\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"Gnrc_Name\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"Tot_Clms\",\"nullable\":true,\"type\":\"integer\"},{\"metadata\":{},\"name\":\"Tot_30day_Fills\",\"nullable\":true,\"type\":\"integer\"},{\"metadata\":{},\"name\":\"Tot_Day_Suply\",\"nullable\":true,\"type\":\"integer\"},{\"metadata\":{},\"name\":\"Tot_Drug_Cst\",\"nullable\":true,\"type\":\"float\"},{\"metadata\":{},\"name\":\"Tot_Benes\",\"nullable\":true,\"type\":\"integer\"},{\"metadata\":{},\"name\":\"GE65_Sprsn_Flag\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"GE65_Tot_Clms\",\"nullable\":true,\"type\":\"integer\"},{\"metadata\":{},\"name\":\"GE65_Tot_30day_Fills\",\"nullable\":true,\"type\":\"integer\"},{\"metadata\":{},\"name\":\"GE65_Tot_Drug_Cst\",\"nullable\":true,\"type\":\"float\"},{\"metadata\":{},\"name\":\"GE65_Tot_Day_Suply\",\"nullable\":true,\"type\":\"integer\"},{\"metadata\":{},\"name\":\"GE65_Bene_Sprsn_Flag\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"GE65_Tot_Benes\",\"nullable\":true,\"type\":\"integer\"}],\"type\":\"struct\"}'\n","\n","schemacsv = StructType.fromJson(json.loads(jschemacsv))"]},{"cell_type":"code","execution_count":2,"id":"e507dba2-3ff4-4fab-af3a-5d3535b4d8a0","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[{"data":{"application/vnd.livy.statement-meta+json":{"execution_finish_time":"2023-09-13T15:48:55.4852985Z","execution_start_time":"2023-09-13T15:48:55.1673625Z","livy_statement_state":"available","parent_msg_id":"c7b6b69b-de85-41e1-b79c-7bc18cd18a5b","queued_time":"2023-09-13T15:46:00.1331402Z","session_id":"f423086e-808a-4b6a-ae13-87c4e1daa3ae","session_start_time":null,"spark_jobs":{"jobs":[],"limit":20,"numbers":{"FAILED":0,"RUNNING":0,"SUCCEEDED":0,"UNKNOWN":0},"rule":"ALL_DESC"},"spark_pool":null,"state":"finished","statement_id":4},"text/plain":["StatementMeta(, f423086e-808a-4b6a-ae13-87c4e1daa3ae, 4, Finished, Available)"]},"metadata":{},"output_type":"display_data"}],"source":["from pyspark.sql.types import *\n","import json\n","\n","jschematable = '{\"fields\":[{\"metadata\":{},\"name\":\"Prscrbr_NPI\",\"nullable\":true,\"type\":\"integer\"},{\"metadata\":{},\"name\":\"Prscrbr_Last_Org_Name\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"Prscrbr_First_Name\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"Prscrbr_City\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"Prscrbr_State_Abrvtn\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"Prscrbr_State_FIPS\",\"nullable\":true,\"type\":\"integer\"},{\"metadata\":{},\"name\":\"Prscrbr_Type\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"Prscrbr_Type_Src\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"Brnd_Name\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"Gnrc_Name\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"Tot_Clms\",\"nullable\":true,\"type\":\"integer\"},{\"metadata\":{},\"name\":\"Tot_30day_Fills\",\"nullable\":true,\"type\":\"integer\"},{\"metadata\":{},\"name\":\"Tot_Day_Suply\",\"nullable\":true,\"type\":\"integer\"},{\"metadata\":{},\"name\":\"Tot_Drug_Cst\",\"nullable\":true,\"type\":\"float\"},{\"metadata\":{},\"name\":\"Tot_Benes\",\"nullable\":true,\"type\":\"integer\"},{\"metadata\":{},\"name\":\"GE65_Sprsn_Flag\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"GE65_Tot_Clms\",\"nullable\":true,\"type\":\"integer\"},{\"metadata\":{},\"name\":\"GE65_Tot_30day_Fills\",\"nullable\":true,\"type\":\"integer\"},{\"metadata\":{},\"name\":\"GE65_Tot_Drug_Cst\",\"nullable\":true,\"type\":\"float\"},{\"metadata\":{},\"name\":\"GE65_Tot_Day_Suply\",\"nullable\":true,\"type\":\"integer\"},{\"metadata\":{},\"name\":\"GE65_Bene_Sprsn_Flag\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"GE65_Tot_Benes\",\"nullable\":true,\"type\":\"integer\"},{\"metadata\":{},\"name\":\"filename\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"year\",\"nullable\":true,\"type\":\"string\"}],\"type\":\"struct\"}'\n","\n","schematable = StructType.fromJson(json.loads(jschematable))"]},{"cell_type":"code","execution_count":3,"id":"ee67a93c-f293-4996-a9fd-edcdcc717c6a","metadata":{"advisor":{"adviceMetadata":"{\"artifactId\":\"9e91bf48-dc09-425f-bb81-db39a923a85c\",\"activityId\":\"f423086e-808a-4b6a-ae13-87c4e1daa3ae\",\"applicationId\":\"application_1694620043924_0001\",\"jobGroupId\":\"5\",\"advices\":{\"info\":9}}"},"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[{"data":{"application/vnd.livy.statement-meta+json":{"execution_finish_time":"2023-09-13T16:10:27.2322776Z","execution_start_time":"2023-09-13T15:48:56.0267489Z","livy_statement_state":"available","parent_msg_id":"b53fb517-5b87-4774-bfcd-aa55dee1f5ef","queued_time":"2023-09-13T15:46:00.1337308Z","session_id":"f423086e-808a-4b6a-ae13-87c4e1daa3ae","session_start_time":null,"spark_jobs":{"jobs":[{"completionTime":"2023-09-13T16:10:26.554GMT","dataRead":4729,"dataWritten":0,"description":"Delta: Job group for statement 5:\nfrom pyspark.sql.types import *\nfrom  pyspark.sql.functions import *\n\ndef loadFullDataFromSource(file_name):\n    sqljoin= \"SELECT 1 as joinyear1, right(substring_index('\" + file_name + \"', '.', 1),4) as fourdigityear\"\n    dfyear = spark.sql(sqljoin)\n    df = spark.read.format(\"csv\").option(\"header\",\"true\").schema(schemacsv).load(\"Files/medicare/demorawfiles/\" + file_name)\n    df = df.withColumn(\"filename\", input_file_name())\n    df = df.withColumn(\"joinyear2\", lit(1))\n    df = df.join(dfyear, df.joinyear2 == dfyear.joinyear1, 'inner')\n    df = df.drop(\"joinyear1\",\"joinyear2\")\n    #display(df) \n    df.write.mode(\"append\").format(\"delta\").option(\"mergeSchema\", \"true\").option(\"schema\", \"schematable\").save(\"Tables/medicarepartd_demo2\")\n    \nfull_files = [\n\n    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2013.csv',\n    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2014.csv',\n    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2015.csv',\n    'Medicare_Part_D_Prescribers_by_Provid...: Compute snapshot for version: 8","displayName":"toString at String.java:2994","jobGroup":"5","jobId":61,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":2,"numSkippedTasks":59,"numTasks":60,"rowCount":50,"stageIds":[99,100,101],"status":"SUCCEEDED","submissionTime":"2023-09-13T16:10:26.529GMT","usageDescription":""},{"completionTime":"2023-09-13T16:10:26.509GMT","dataRead":19074,"dataWritten":4729,"description":"Delta: Job group for statement 5:\nfrom pyspark.sql.types import *\nfrom  pyspark.sql.functions import *\n\ndef loadFullDataFromSource(file_name):\n    sqljoin= \"SELECT 1 as joinyear1, right(substring_index('\" + file_name + \"', '.', 1),4) as fourdigityear\"\n    dfyear = spark.sql(sqljoin)\n    df = spark.read.format(\"csv\").option(\"header\",\"true\").schema(schemacsv).load(\"Files/medicare/demorawfiles/\" + file_name)\n    df = df.withColumn(\"filename\", input_file_name())\n    df = df.withColumn(\"joinyear2\", lit(1))\n    df = df.join(dfyear, df.joinyear2 == dfyear.joinyear1, 'inner')\n    df = df.drop(\"joinyear1\",\"joinyear2\")\n    #display(df) \n    df.write.mode(\"append\").format(\"delta\").option(\"mergeSchema\", \"true\").option(\"schema\", \"schematable\").save(\"Tables/medicarepartd_demo2\")\n    \nfull_files = [\n\n    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2013.csv',\n    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2014.csv',\n    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2015.csv',\n    'Medicare_Part_D_Prescribers_by_Provid...: Compute snapshot for version: 8","displayName":"toString at String.java:2994","jobGroup":"5","jobId":60,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":50,"numCompletedStages":1,"numCompletedTasks":50,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":9,"numTasks":59,"rowCount":70,"stageIds":[97,98],"status":"SUCCEEDED","submissionTime":"2023-09-13T16:10:26.279GMT","usageDescription":""},{"completionTime":"2023-09-13T16:10:26.165GMT","dataRead":26548,"dataWritten":19074,"description":"Delta: Job group for statement 5:\nfrom pyspark.sql.types import *\nfrom  pyspark.sql.functions import *\n\ndef loadFullDataFromSource(file_name):\n    sqljoin= \"SELECT 1 as joinyear1, right(substring_index('\" + file_name + \"', '.', 1),4) as fourdigityear\"\n    dfyear = spark.sql(sqljoin)\n    df = spark.read.format(\"csv\").option(\"header\",\"true\").schema(schemacsv).load(\"Files/medicare/demorawfiles/\" + file_name)\n    df = df.withColumn(\"filename\", input_file_name())\n    df = df.withColumn(\"joinyear2\", lit(1))\n    df = df.join(dfyear, df.joinyear2 == dfyear.joinyear1, 'inner')\n    df = df.drop(\"joinyear1\",\"joinyear2\")\n    #display(df) \n    df.write.mode(\"append\").format(\"delta\").option(\"mergeSchema\", \"true\").option(\"schema\", \"schematable\").save(\"Tables/medicarepartd_demo2\")\n    \nfull_files = [\n\n    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2013.csv',\n    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2014.csv',\n    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2015.csv',\n    'Medicare_Part_D_Prescribers_by_Provid...: Compute snapshot for version: 8","displayName":"toString at String.java:2994","jobGroup":"5","jobId":59,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":9,"numCompletedStages":1,"numCompletedTasks":9,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":9,"rowCount":40,"stageIds":[96],"status":"SUCCEEDED","submissionTime":"2023-09-13T16:10:26.070GMT","usageDescription":""},{"completionTime":"2023-09-13T16:10:25.623GMT","dataRead":1766088795,"dataWritten":514673745,"description":"Job group for statement 5:\nfrom pyspark.sql.types import *\nfrom  pyspark.sql.functions import *\n\ndef loadFullDataFromSource(file_name):\n    sqljoin= \"SELECT 1 as joinyear1, right(substring_index('\" + file_name + \"', '.', 1),4) as fourdigityear\"\n    dfyear = spark.sql(sqljoin)\n    df = spark.read.format(\"csv\").option(\"header\",\"true\").schema(schemacsv).load(\"Files/medicare/demorawfiles/\" + file_name)\n    df = df.withColumn(\"filename\", input_file_name())\n    df = df.withColumn(\"joinyear2\", lit(1))\n    df = df.join(dfyear, df.joinyear2 == dfyear.joinyear1, 'inner')\n    df = df.drop(\"joinyear1\",\"joinyear2\")\n    #display(df) \n    df.write.mode(\"append\").format(\"delta\").option(\"mergeSchema\", \"true\").option(\"schema\", \"schematable\").save(\"Tables/medicarepartd_demo2\")\n    \nfull_files = [\n\n    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2013.csv',\n    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2014.csv',\n    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2015.csv',\n    'Medicare_Part_D_Prescribers_by_Provid...","displayName":"save at NativeMethodAccessorImpl.java:0","jobGroup":"5","jobId":58,"killedTasksSummary":{},"name":"save at NativeMethodAccessorImpl.java:0","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":27,"numTasks":28,"rowCount":50463724,"stageIds":[94,95],"status":"SUCCEEDED","submissionTime":"2023-09-13T16:08:18.329GMT","usageDescription":""},{"completionTime":"2023-09-13T16:08:18.280GMT","dataRead":3624738993,"dataWritten":1766088795,"description":"Job group for statement 5:\nfrom pyspark.sql.types import *\nfrom  pyspark.sql.functions import *\n\ndef loadFullDataFromSource(file_name):\n    sqljoin= \"SELECT 1 as joinyear1, right(substring_index('\" + file_name + \"', '.', 1),4) as fourdigityear\"\n    dfyear = spark.sql(sqljoin)\n    df = spark.read.format(\"csv\").option(\"header\",\"true\").schema(schemacsv).load(\"Files/medicare/demorawfiles/\" + file_name)\n    df = df.withColumn(\"filename\", input_file_name())\n    df = df.withColumn(\"joinyear2\", lit(1))\n    df = df.join(dfyear, df.joinyear2 == dfyear.joinyear1, 'inner')\n    df = df.drop(\"joinyear1\",\"joinyear2\")\n    #display(df) \n    df.write.mode(\"append\").format(\"delta\").option(\"mergeSchema\", \"true\").option(\"schema\", \"schematable\").save(\"Tables/medicarepartd_demo2\")\n    \nfull_files = [\n\n    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2013.csv',\n    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2014.csv',\n    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2015.csv',\n    'Medicare_Part_D_Prescribers_by_Provid...","displayName":"save at NativeMethodAccessorImpl.java:0","jobGroup":"5","jobId":57,"killedTasksSummary":{},"name":"save at NativeMethodAccessorImpl.java:0","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":27,"numCompletedStages":1,"numCompletedTasks":27,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":27,"rowCount":50463724,"stageIds":[93],"status":"SUCCEEDED","submissionTime":"2023-09-13T16:07:56.067GMT","usageDescription":""},{"completionTime":"2023-09-13T16:07:55.801GMT","dataRead":4714,"dataWritten":0,"description":"Delta: Job group for statement 5:\nfrom pyspark.sql.types import *\nfrom  pyspark.sql.functions import *\n\ndef loadFullDataFromSource(file_name):\n    sqljoin= \"SELECT 1 as joinyear1, right(substring_index('\" + file_name + \"', '.', 1),4) as fourdigityear\"\n    dfyear = spark.sql(sqljoin)\n    df = spark.read.format(\"csv\").option(\"header\",\"true\").schema(schemacsv).load(\"Files/medicare/demorawfiles/\" + file_name)\n    df = df.withColumn(\"filename\", input_file_name())\n    df = df.withColumn(\"joinyear2\", lit(1))\n    df = df.join(dfyear, df.joinyear2 == dfyear.joinyear1, 'inner')\n    df = df.drop(\"joinyear1\",\"joinyear2\")\n    #display(df) \n    df.write.mode(\"append\").format(\"delta\").option(\"mergeSchema\", \"true\").option(\"schema\", \"schematable\").save(\"Tables/medicarepartd_demo2\")\n    \nfull_files = [\n\n    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2013.csv',\n    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2014.csv',\n    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2015.csv',\n    'Medicare_Part_D_Prescribers_by_Provid...: Compute snapshot for version: 7","displayName":"toString at String.java:2994","jobGroup":"5","jobId":55,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":2,"numSkippedTasks":58,"numTasks":59,"rowCount":50,"stageIds":[89,90,91],"status":"SUCCEEDED","submissionTime":"2023-09-13T16:07:55.778GMT","usageDescription":""},{"completionTime":"2023-09-13T16:07:55.765GMT","dataRead":16999,"dataWritten":4714,"description":"Delta: Job group for statement 5:\nfrom pyspark.sql.types import *\nfrom  pyspark.sql.functions import *\n\ndef loadFullDataFromSource(file_name):\n    sqljoin= \"SELECT 1 as joinyear1, right(substring_index('\" + file_name + \"', '.', 1),4) as fourdigityear\"\n    dfyear = spark.sql(sqljoin)\n    df = spark.read.format(\"csv\").option(\"header\",\"true\").schema(schemacsv).load(\"Files/medicare/demorawfiles/\" + file_name)\n    df = df.withColumn(\"filename\", input_file_name())\n    df = df.withColumn(\"joinyear2\", lit(1))\n    df = df.join(dfyear, df.joinyear2 == dfyear.joinyear1, 'inner')\n    df = df.drop(\"joinyear1\",\"joinyear2\")\n    #display(df) \n    df.write.mode(\"append\").format(\"delta\").option(\"mergeSchema\", \"true\").option(\"schema\", \"schematable\").save(\"Tables/medicarepartd_demo2\")\n    \nfull_files = [\n\n    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2013.csv',\n    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2014.csv',\n    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2015.csv',\n    'Medicare_Part_D_Prescribers_by_Provid...: Compute snapshot for version: 7","displayName":"toString at String.java:2994","jobGroup":"5","jobId":54,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":50,"numCompletedStages":1,"numCompletedTasks":50,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":8,"numTasks":58,"rowCount":68,"stageIds":[88,87],"status":"SUCCEEDED","submissionTime":"2023-09-13T16:07:55.514GMT","usageDescription":""},{"completionTime":"2023-09-13T16:07:55.349GMT","dataRead":23839,"dataWritten":16999,"description":"Delta: Job group for statement 5:\nfrom pyspark.sql.types import *\nfrom  pyspark.sql.functions import *\n\ndef loadFullDataFromSource(file_name):\n    sqljoin= \"SELECT 1 as joinyear1, right(substring_index('\" + file_name + \"', '.', 1),4) as fourdigityear\"\n    dfyear = spark.sql(sqljoin)\n    df = spark.read.format(\"csv\").option(\"header\",\"true\").schema(schemacsv).load(\"Files/medicare/demorawfiles/\" + file_name)\n    df = df.withColumn(\"filename\", input_file_name())\n    df = df.withColumn(\"joinyear2\", lit(1))\n    df = df.join(dfyear, df.joinyear2 == dfyear.joinyear1, 'inner')\n    df = df.drop(\"joinyear1\",\"joinyear2\")\n    #display(df) \n    df.write.mode(\"append\").format(\"delta\").option(\"mergeSchema\", \"true\").option(\"schema\", \"schematable\").save(\"Tables/medicarepartd_demo2\")\n    \nfull_files = [\n\n    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2013.csv',\n    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2014.csv',\n    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2015.csv',\n    'Medicare_Part_D_Prescribers_by_Provid...: Compute snapshot for version: 7","displayName":"toString at String.java:2994","jobGroup":"5","jobId":53,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":8,"numCompletedStages":1,"numCompletedTasks":8,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":8,"rowCount":36,"stageIds":[86],"status":"SUCCEEDED","submissionTime":"2023-09-13T16:07:55.248GMT","usageDescription":""},{"completionTime":"2023-09-13T16:07:54.800GMT","dataRead":1763065756,"dataWritten":512893122,"description":"Job group for statement 5:\nfrom pyspark.sql.types import *\nfrom  pyspark.sql.functions import *\n\ndef loadFullDataFromSource(file_name):\n    sqljoin= \"SELECT 1 as joinyear1, right(substring_index('\" + file_name + \"', '.', 1),4) as fourdigityear\"\n    dfyear = spark.sql(sqljoin)\n    df = spark.read.format(\"csv\").option(\"header\",\"true\").schema(schemacsv).load(\"Files/medicare/demorawfiles/\" + file_name)\n    df = df.withColumn(\"filename\", input_file_name())\n    df = df.withColumn(\"joinyear2\", lit(1))\n    df = df.join(dfyear, df.joinyear2 == dfyear.joinyear1, 'inner')\n    df = df.drop(\"joinyear1\",\"joinyear2\")\n    #display(df) \n    df.write.mode(\"append\").format(\"delta\").option(\"mergeSchema\", \"true\").option(\"schema\", \"schematable\").save(\"Tables/medicarepartd_demo2\")\n    \nfull_files = [\n\n    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2013.csv',\n    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2014.csv',\n    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2015.csv',\n    'Medicare_Part_D_Prescribers_by_Provid...","displayName":"save at NativeMethodAccessorImpl.java:0","jobGroup":"5","jobId":52,"killedTasksSummary":{},"name":"save at NativeMethodAccessorImpl.java:0","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":27,"numTasks":28,"rowCount":50419458,"stageIds":[84,85],"status":"SUCCEEDED","submissionTime":"2023-09-13T16:05:51.947GMT","usageDescription":""},{"completionTime":"2023-09-13T16:05:51.901GMT","dataRead":3619495437,"dataWritten":1763065756,"description":"Job group for statement 5:\nfrom pyspark.sql.types import *\nfrom  pyspark.sql.functions import *\n\ndef loadFullDataFromSource(file_name):\n    sqljoin= \"SELECT 1 as joinyear1, right(substring_index('\" + file_name + \"', '.', 1),4) as fourdigityear\"\n    dfyear = spark.sql(sqljoin)\n    df = spark.read.format(\"csv\").option(\"header\",\"true\").schema(schemacsv).load(\"Files/medicare/demorawfiles/\" + file_name)\n    df = df.withColumn(\"filename\", input_file_name())\n    df = df.withColumn(\"joinyear2\", lit(1))\n    df = df.join(dfyear, df.joinyear2 == dfyear.joinyear1, 'inner')\n    df = df.drop(\"joinyear1\",\"joinyear2\")\n    #display(df) \n    df.write.mode(\"append\").format(\"delta\").option(\"mergeSchema\", \"true\").option(\"schema\", \"schematable\").save(\"Tables/medicarepartd_demo2\")\n    \nfull_files = [\n\n    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2013.csv',\n    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2014.csv',\n    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2015.csv',\n    'Medicare_Part_D_Prescribers_by_Provid...","displayName":"save at NativeMethodAccessorImpl.java:0","jobGroup":"5","jobId":51,"killedTasksSummary":{},"name":"save at NativeMethodAccessorImpl.java:0","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":27,"numCompletedStages":1,"numCompletedTasks":27,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":27,"rowCount":50419458,"stageIds":[83],"status":"SUCCEEDED","submissionTime":"2023-09-13T16:05:29.208GMT","usageDescription":""},{"completionTime":"2023-09-13T16:05:28.928GMT","dataRead":4699,"dataWritten":0,"description":"Delta: Job group for statement 5:\nfrom pyspark.sql.types import *\nfrom  pyspark.sql.functions import *\n\ndef loadFullDataFromSource(file_name):\n    sqljoin= \"SELECT 1 as joinyear1, right(substring_index('\" + file_name + \"', '.', 1),4) as fourdigityear\"\n    dfyear = spark.sql(sqljoin)\n    df = spark.read.format(\"csv\").option(\"header\",\"true\").schema(schemacsv).load(\"Files/medicare/demorawfiles/\" + file_name)\n    df = df.withColumn(\"filename\", input_file_name())\n    df = df.withColumn(\"joinyear2\", lit(1))\n    df = df.join(dfyear, df.joinyear2 == dfyear.joinyear1, 'inner')\n    df = df.drop(\"joinyear1\",\"joinyear2\")\n    #display(df) \n    df.write.mode(\"append\").format(\"delta\").option(\"mergeSchema\", \"true\").option(\"schema\", \"schematable\").save(\"Tables/medicarepartd_demo2\")\n    \nfull_files = [\n\n    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2013.csv',\n    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2014.csv',\n    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2015.csv',\n    'Medicare_Part_D_Prescribers_by_Provid...: Compute snapshot for version: 6","displayName":"toString at String.java:2994","jobGroup":"5","jobId":49,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":2,"numSkippedTasks":57,"numTasks":58,"rowCount":50,"stageIds":[81,79,80],"status":"SUCCEEDED","submissionTime":"2023-09-13T16:05:28.904GMT","usageDescription":""},{"completionTime":"2023-09-13T16:05:28.887GMT","dataRead":14955,"dataWritten":4699,"description":"Delta: Job group for statement 5:\nfrom pyspark.sql.types import *\nfrom  pyspark.sql.functions import *\n\ndef loadFullDataFromSource(file_name):\n    sqljoin= \"SELECT 1 as joinyear1, right(substring_index('\" + file_name + \"', '.', 1),4) as fourdigityear\"\n    dfyear = spark.sql(sqljoin)\n    df = spark.read.format(\"csv\").option(\"header\",\"true\").schema(schemacsv).load(\"Files/medicare/demorawfiles/\" + file_name)\n    df = df.withColumn(\"filename\", input_file_name())\n    df = df.withColumn(\"joinyear2\", lit(1))\n    df = df.join(dfyear, df.joinyear2 == dfyear.joinyear1, 'inner')\n    df = df.drop(\"joinyear1\",\"joinyear2\")\n    #display(df) \n    df.write.mode(\"append\").format(\"delta\").option(\"mergeSchema\", \"true\").option(\"schema\", \"schematable\").save(\"Tables/medicarepartd_demo2\")\n    \nfull_files = [\n\n    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2013.csv',\n    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2014.csv',\n    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2015.csv',\n    'Medicare_Part_D_Prescribers_by_Provid...: Compute snapshot for version: 6","displayName":"toString at String.java:2994","jobGroup":"5","jobId":48,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":50,"numCompletedStages":1,"numCompletedTasks":50,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":7,"numTasks":57,"rowCount":66,"stageIds":[78,77],"status":"SUCCEEDED","submissionTime":"2023-09-13T16:05:28.615GMT","usageDescription":""},{"completionTime":"2023-09-13T16:05:28.501GMT","dataRead":21145,"dataWritten":14955,"description":"Delta: Job group for statement 5:\nfrom pyspark.sql.types import *\nfrom  pyspark.sql.functions import *\n\ndef loadFullDataFromSource(file_name):\n    sqljoin= \"SELECT 1 as joinyear1, right(substring_index('\" + file_name + \"', '.', 1),4) as fourdigityear\"\n    dfyear = spark.sql(sqljoin)\n    df = spark.read.format(\"csv\").option(\"header\",\"true\").schema(schemacsv).load(\"Files/medicare/demorawfiles/\" + file_name)\n    df = df.withColumn(\"filename\", input_file_name())\n    df = df.withColumn(\"joinyear2\", lit(1))\n    df = df.join(dfyear, df.joinyear2 == dfyear.joinyear1, 'inner')\n    df = df.drop(\"joinyear1\",\"joinyear2\")\n    #display(df) \n    df.write.mode(\"append\").format(\"delta\").option(\"mergeSchema\", \"true\").option(\"schema\", \"schematable\").save(\"Tables/medicarepartd_demo2\")\n    \nfull_files = [\n\n    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2013.csv',\n    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2014.csv',\n    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2015.csv',\n    'Medicare_Part_D_Prescribers_by_Provid...: Compute snapshot for version: 6","displayName":"toString at String.java:2994","jobGroup":"5","jobId":47,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":7,"numCompletedStages":1,"numCompletedTasks":7,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":7,"rowCount":32,"stageIds":[76],"status":"SUCCEEDED","submissionTime":"2023-09-13T16:05:28.383GMT","usageDescription":""},{"completionTime":"2023-09-13T16:05:27.937GMT","dataRead":1778389570,"dataWritten":516065442,"description":"Job group for statement 5:\nfrom pyspark.sql.types import *\nfrom  pyspark.sql.functions import *\n\ndef loadFullDataFromSource(file_name):\n    sqljoin= \"SELECT 1 as joinyear1, right(substring_index('\" + file_name + \"', '.', 1),4) as fourdigityear\"\n    dfyear = spark.sql(sqljoin)\n    df = spark.read.format(\"csv\").option(\"header\",\"true\").schema(schemacsv).load(\"Files/medicare/demorawfiles/\" + file_name)\n    df = df.withColumn(\"filename\", input_file_name())\n    df = df.withColumn(\"joinyear2\", lit(1))\n    df = df.join(dfyear, df.joinyear2 == dfyear.joinyear1, 'inner')\n    df = df.drop(\"joinyear1\",\"joinyear2\")\n    #display(df) \n    df.write.mode(\"append\").format(\"delta\").option(\"mergeSchema\", \"true\").option(\"schema\", \"schematable\").save(\"Tables/medicarepartd_demo2\")\n    \nfull_files = [\n\n    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2013.csv',\n    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2014.csv',\n    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2015.csv',\n    'Medicare_Part_D_Prescribers_by_Provid...","displayName":"save at NativeMethodAccessorImpl.java:0","jobGroup":"5","jobId":46,"killedTasksSummary":{},"name":"save at NativeMethodAccessorImpl.java:0","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":28,"numTasks":29,"rowCount":50803740,"stageIds":[74,75],"status":"SUCCEEDED","submissionTime":"2023-09-13T16:03:22.061GMT","usageDescription":""},{"completionTime":"2023-09-13T16:03:22.016GMT","dataRead":3640459324,"dataWritten":1778389570,"description":"Job group for statement 5:\nfrom pyspark.sql.types import *\nfrom  pyspark.sql.functions import *\n\ndef loadFullDataFromSource(file_name):\n    sqljoin= \"SELECT 1 as joinyear1, right(substring_index('\" + file_name + \"', '.', 1),4) as fourdigityear\"\n    dfyear = spark.sql(sqljoin)\n    df = spark.read.format(\"csv\").option(\"header\",\"true\").schema(schemacsv).load(\"Files/medicare/demorawfiles/\" + file_name)\n    df = df.withColumn(\"filename\", input_file_name())\n    df = df.withColumn(\"joinyear2\", lit(1))\n    df = df.join(dfyear, df.joinyear2 == dfyear.joinyear1, 'inner')\n    df = df.drop(\"joinyear1\",\"joinyear2\")\n    #display(df) \n    df.write.mode(\"append\").format(\"delta\").option(\"mergeSchema\", \"true\").option(\"schema\", \"schematable\").save(\"Tables/medicarepartd_demo2\")\n    \nfull_files = [\n\n    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2013.csv',\n    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2014.csv',\n    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2015.csv',\n    'Medicare_Part_D_Prescribers_by_Provid...","displayName":"save at NativeMethodAccessorImpl.java:0","jobGroup":"5","jobId":45,"killedTasksSummary":{},"name":"save at NativeMethodAccessorImpl.java:0","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":28,"numCompletedStages":1,"numCompletedTasks":28,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":28,"rowCount":50803740,"stageIds":[73],"status":"SUCCEEDED","submissionTime":"2023-09-13T16:02:59.939GMT","usageDescription":""},{"completionTime":"2023-09-13T16:02:59.676GMT","dataRead":4684,"dataWritten":0,"description":"Delta: Job group for statement 5:\nfrom pyspark.sql.types import *\nfrom  pyspark.sql.functions import *\n\ndef loadFullDataFromSource(file_name):\n    sqljoin= \"SELECT 1 as joinyear1, right(substring_index('\" + file_name + \"', '.', 1),4) as fourdigityear\"\n    dfyear = spark.sql(sqljoin)\n    df = spark.read.format(\"csv\").option(\"header\",\"true\").schema(schemacsv).load(\"Files/medicare/demorawfiles/\" + file_name)\n    df = df.withColumn(\"filename\", input_file_name())\n    df = df.withColumn(\"joinyear2\", lit(1))\n    df = df.join(dfyear, df.joinyear2 == dfyear.joinyear1, 'inner')\n    df = df.drop(\"joinyear1\",\"joinyear2\")\n    #display(df) \n    df.write.mode(\"append\").format(\"delta\").option(\"mergeSchema\", \"true\").option(\"schema\", \"schematable\").save(\"Tables/medicarepartd_demo2\")\n    \nfull_files = [\n\n    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2013.csv',\n    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2014.csv',\n    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2015.csv',\n    'Medicare_Part_D_Prescribers_by_Provid...: Compute snapshot for version: 5","displayName":"toString at String.java:2994","jobGroup":"5","jobId":43,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":2,"numSkippedTasks":56,"numTasks":57,"rowCount":50,"stageIds":[70,71,69],"status":"SUCCEEDED","submissionTime":"2023-09-13T16:02:59.644GMT","usageDescription":""},{"completionTime":"2023-09-13T16:02:59.594GMT","dataRead":12871,"dataWritten":4684,"description":"Delta: Job group for statement 5:\nfrom pyspark.sql.types import *\nfrom  pyspark.sql.functions import *\n\ndef loadFullDataFromSource(file_name):\n    sqljoin= \"SELECT 1 as joinyear1, right(substring_index('\" + file_name + \"', '.', 1),4) as fourdigityear\"\n    dfyear = spark.sql(sqljoin)\n    df = spark.read.format(\"csv\").option(\"header\",\"true\").schema(schemacsv).load(\"Files/medicare/demorawfiles/\" + file_name)\n    df = df.withColumn(\"filename\", input_file_name())\n    df = df.withColumn(\"joinyear2\", lit(1))\n    df = df.join(dfyear, df.joinyear2 == dfyear.joinyear1, 'inner')\n    df = df.drop(\"joinyear1\",\"joinyear2\")\n    #display(df) \n    df.write.mode(\"append\").format(\"delta\").option(\"mergeSchema\", \"true\").option(\"schema\", \"schematable\").save(\"Tables/medicarepartd_demo2\")\n    \nfull_files = [\n\n    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2013.csv',\n    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2014.csv',\n    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2015.csv',\n    'Medicare_Part_D_Prescribers_by_Provid...: Compute snapshot for version: 5","displayName":"toString at String.java:2994","jobGroup":"5","jobId":42,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":50,"numCompletedStages":1,"numCompletedTasks":50,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":6,"numTasks":56,"rowCount":64,"stageIds":[67,68],"status":"SUCCEEDED","submissionTime":"2023-09-13T16:02:59.364GMT","usageDescription":""},{"completionTime":"2023-09-13T16:02:59.258GMT","dataRead":18437,"dataWritten":12871,"description":"Delta: Job group for statement 5:\nfrom pyspark.sql.types import *\nfrom  pyspark.sql.functions import *\n\ndef loadFullDataFromSource(file_name):\n    sqljoin= \"SELECT 1 as joinyear1, right(substring_index('\" + file_name + \"', '.', 1),4) as fourdigityear\"\n    dfyear = spark.sql(sqljoin)\n    df = spark.read.format(\"csv\").option(\"header\",\"true\").schema(schemacsv).load(\"Files/medicare/demorawfiles/\" + file_name)\n    df = df.withColumn(\"filename\", input_file_name())\n    df = df.withColumn(\"joinyear2\", lit(1))\n    df = df.join(dfyear, df.joinyear2 == dfyear.joinyear1, 'inner')\n    df = df.drop(\"joinyear1\",\"joinyear2\")\n    #display(df) \n    df.write.mode(\"append\").format(\"delta\").option(\"mergeSchema\", \"true\").option(\"schema\", \"schematable\").save(\"Tables/medicarepartd_demo2\")\n    \nfull_files = [\n\n    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2013.csv',\n    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2014.csv',\n    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2015.csv',\n    'Medicare_Part_D_Prescribers_by_Provid...: Compute snapshot for version: 5","displayName":"toString at String.java:2994","jobGroup":"5","jobId":41,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":6,"numCompletedStages":1,"numCompletedTasks":6,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":6,"rowCount":28,"stageIds":[66],"status":"SUCCEEDED","submissionTime":"2023-09-13T16:02:59.163GMT","usageDescription":""},{"completionTime":"2023-09-13T16:02:58.738GMT","dataRead":1772559087,"dataWritten":511298045,"description":"Job group for statement 5:\nfrom pyspark.sql.types import *\nfrom  pyspark.sql.functions import *\n\ndef loadFullDataFromSource(file_name):\n    sqljoin= \"SELECT 1 as joinyear1, right(substring_index('\" + file_name + \"', '.', 1),4) as fourdigityear\"\n    dfyear = spark.sql(sqljoin)\n    df = spark.read.format(\"csv\").option(\"header\",\"true\").schema(schemacsv).load(\"Files/medicare/demorawfiles/\" + file_name)\n    df = df.withColumn(\"filename\", input_file_name())\n    df = df.withColumn(\"joinyear2\", lit(1))\n    df = df.join(dfyear, df.joinyear2 == dfyear.joinyear1, 'inner')\n    df = df.drop(\"joinyear1\",\"joinyear2\")\n    #display(df) \n    df.write.mode(\"append\").format(\"delta\").option(\"mergeSchema\", \"true\").option(\"schema\", \"schematable\").save(\"Tables/medicarepartd_demo2\")\n    \nfull_files = [\n\n    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2013.csv',\n    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2014.csv',\n    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2015.csv',\n    'Medicare_Part_D_Prescribers_by_Provid...","displayName":"save at NativeMethodAccessorImpl.java:0","jobGroup":"5","jobId":40,"killedTasksSummary":{},"name":"save at NativeMethodAccessorImpl.java:0","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":27,"numTasks":28,"rowCount":50623200,"stageIds":[64,65],"status":"SUCCEEDED","submissionTime":"2023-09-13T16:00:51.728GMT","usageDescription":""},{"completionTime":"2023-09-13T16:00:51.676GMT","dataRead":3623598831,"dataWritten":1772559087,"description":"Job group for statement 5:\nfrom pyspark.sql.types import *\nfrom  pyspark.sql.functions import *\n\ndef loadFullDataFromSource(file_name):\n    sqljoin= \"SELECT 1 as joinyear1, right(substring_index('\" + file_name + \"', '.', 1),4) as fourdigityear\"\n    dfyear = spark.sql(sqljoin)\n    df = spark.read.format(\"csv\").option(\"header\",\"true\").schema(schemacsv).load(\"Files/medicare/demorawfiles/\" + file_name)\n    df = df.withColumn(\"filename\", input_file_name())\n    df = df.withColumn(\"joinyear2\", lit(1))\n    df = df.join(dfyear, df.joinyear2 == dfyear.joinyear1, 'inner')\n    df = df.drop(\"joinyear1\",\"joinyear2\")\n    #display(df) \n    df.write.mode(\"append\").format(\"delta\").option(\"mergeSchema\", \"true\").option(\"schema\", \"schematable\").save(\"Tables/medicarepartd_demo2\")\n    \nfull_files = [\n\n    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2013.csv',\n    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2014.csv',\n    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2015.csv',\n    'Medicare_Part_D_Prescribers_by_Provid...","displayName":"save at NativeMethodAccessorImpl.java:0","jobGroup":"5","jobId":39,"killedTasksSummary":{},"name":"save at NativeMethodAccessorImpl.java:0","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":27,"numCompletedStages":1,"numCompletedTasks":27,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":27,"rowCount":50623200,"stageIds":[63],"status":"SUCCEEDED","submissionTime":"2023-09-13T16:00:30.231GMT","usageDescription":""}],"limit":20,"numbers":{"FAILED":0,"RUNNING":0,"SUCCEEDED":45,"UNKNOWN":0},"rule":"ALL_DESC"},"spark_pool":null,"state":"finished","statement_id":5},"text/plain":["StatementMeta(, f423086e-808a-4b6a-ae13-87c4e1daa3ae, 5, Finished, Available)"]},"metadata":{},"output_type":"display_data"}],"source":["from pyspark.sql.types import *\n","from  pyspark.sql.functions import *\n","\n","def loadFullDataFromSource(file_name):\n","    sqljoin= \"SELECT 1 as joinyear1, right(substring_index('\" + file_name + \"', '.', 1),4) as fourdigityear\"\n","    dfyear = spark.sql(sqljoin)\n","    df = spark.read.format(\"csv\").option(\"header\",\"true\").schema(schemacsv).load(\"Files/medicare/raw/\" + file_name)\n","    df = df.withColumn(\"filename\", input_file_name())\n","    df = df.withColumn(\"joinyear2\", lit(1))\n","    df = df.join(dfyear, df.joinyear2 == dfyear.joinyear1, 'inner')\n","    df = df.drop(\"joinyear1\",\"joinyear2\")\n","    #display(df) \n","    df.write.mode(\"append\").format(\"delta\").option(\"mergeSchema\", \"true\").option(\"schema\", \"schematable\").save(\"Tables/medicarepartd\")\n","    \n","full_files = [\n","\n","    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2013.csv',\n","    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2014.csv',\n","    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2015.csv',\n","    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2016.csv',\n","    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2017.csv',\n","    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_2018.csv',\n","    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_2019.csv',\n","    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_2020.csv',\n","    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_2021.csv'\n","    ]\n","\n","for file in full_files:\n","    loadFullDataFromSource(file)"]}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"display_name":"Synapse PySpark","name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"host":{"synapse_widget":{"state":{"66515eeb-bab1-4fc7-8452-c677ee4f6d59":{"persist_state":{"view":{"chartOptions":{"aggregationType":"sum","binsNumber":10,"categoryFieldKeys":["1"],"chartType":"bar","isStacked":false,"seriesFieldKeys":["0"],"wordFrequency":"-1"},"tableOptions":{},"type":"details"}},"sync_state":{"isSummary":false,"language":"scala","table":{"rows":[{"0":"1","1":"2018","index":1}],"schema":[{"key":"0","name":"joinyear","type":"int"},{"key":"1","name":"fourdigityear","type":"string"}],"truncated":false}},"type":"Synapse.DataFrame"}},"token":"f64b5c25-b19f-48b3-ad55-fd2c46f301e7"}},"language":"python","ms_spell_check":{"ms_spell_check_language":"en"}},"notebook_environment":{},"nteract":{"version":"nteract-front-end@1.0.0"},"save_output":true,"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{},"enableDebugMode":false}},"synapse_widget":{"state":{},"version":"0.1"},"trident":{"lakehouse":{"default_lakehouse":"2a021a4f-3660-4f82-91d8-49bbf7dec376","known_lakehouses":[{"id":"2a021a4f-3660-4f82-91d8-49bbf7dec376"}]}},"widgets":{}},"nbformat":4,"nbformat_minor":5}
