{"cells":[{"cell_type":"code","source":["from pyspark.sql.types import *\n","import json\n","\n","jschemacsv = '{\"fields\":[{\"metadata\":{},\"name\":\"Prscrbr_NPI\",\"nullable\":true,\"type\":\"integer\"},{\"metadata\":{},\"name\":\"Prscrbr_Last_Org_Name\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"Prscrbr_First_Name\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"Prscrbr_City\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"Prscrbr_State_Abrvtn\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"Prscrbr_State_FIPS\",\"nullable\":true,\"type\":\"integer\"},{\"metadata\":{},\"name\":\"Prscrbr_Type\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"Prscrbr_Type_Src\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"Brnd_Name\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"Gnrc_Name\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"Tot_Clms\",\"nullable\":true,\"type\":\"integer\"},{\"metadata\":{},\"name\":\"Tot_30day_Fills\",\"nullable\":true,\"type\":\"integer\"},{\"metadata\":{},\"name\":\"Tot_Day_Suply\",\"nullable\":true,\"type\":\"integer\"},{\"metadata\":{},\"name\":\"Tot_Drug_Cst\",\"nullable\":true,\"type\":\"float\"},{\"metadata\":{},\"name\":\"Tot_Benes\",\"nullable\":true,\"type\":\"integer\"},{\"metadata\":{},\"name\":\"GE65_Sprsn_Flag\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"GE65_Tot_Clms\",\"nullable\":true,\"type\":\"integer\"},{\"metadata\":{},\"name\":\"GE65_Tot_30day_Fills\",\"nullable\":true,\"type\":\"integer\"},{\"metadata\":{},\"name\":\"GE65_Tot_Drug_Cst\",\"nullable\":true,\"type\":\"float\"},{\"metadata\":{},\"name\":\"GE65_Tot_Day_Suply\",\"nullable\":true,\"type\":\"integer\"},{\"metadata\":{},\"name\":\"GE65_Bene_Sprsn_Flag\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"GE65_Tot_Benes\",\"nullable\":true,\"type\":\"integer\"}],\"type\":\"struct\"}'\n","\n","schemacsv = StructType.fromJson(json.loads(jschemacsv))"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"f423086e-808a-4b6a-ae13-87c4e1daa3ae","statement_id":3,"state":"finished","livy_statement_state":"available","queued_time":"2023-09-13T15:46:00.1325105Z","session_start_time":"2023-09-13T15:46:00.848683Z","execution_start_time":"2023-09-13T15:48:52.4033724Z","execution_finish_time":"2023-09-13T15:48:54.6386952Z","spark_jobs":{"numbers":{"SUCCEEDED":0,"UNKNOWN":0,"FAILED":0,"RUNNING":0},"jobs":[],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"cffba9e7-f2fa-43cf-a1c9-2685857e6ce1"},"text/plain":"StatementMeta(, f423086e-808a-4b6a-ae13-87c4e1daa3ae, 3, Finished, Available)"},"metadata":{}}],"execution_count":1,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"5b8ac155-4305-44e0-bee5-69656cdbd6cd"},{"cell_type":"code","source":["from pyspark.sql.types import *\n","import json\n","\n","jschematable = '{\"fields\":[{\"metadata\":{},\"name\":\"Prscrbr_NPI\",\"nullable\":true,\"type\":\"integer\"},{\"metadata\":{},\"name\":\"Prscrbr_Last_Org_Name\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"Prscrbr_First_Name\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"Prscrbr_City\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"Prscrbr_State_Abrvtn\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"Prscrbr_State_FIPS\",\"nullable\":true,\"type\":\"integer\"},{\"metadata\":{},\"name\":\"Prscrbr_Type\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"Prscrbr_Type_Src\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"Brnd_Name\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"Gnrc_Name\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"Tot_Clms\",\"nullable\":true,\"type\":\"integer\"},{\"metadata\":{},\"name\":\"Tot_30day_Fills\",\"nullable\":true,\"type\":\"integer\"},{\"metadata\":{},\"name\":\"Tot_Day_Suply\",\"nullable\":true,\"type\":\"integer\"},{\"metadata\":{},\"name\":\"Tot_Drug_Cst\",\"nullable\":true,\"type\":\"float\"},{\"metadata\":{},\"name\":\"Tot_Benes\",\"nullable\":true,\"type\":\"integer\"},{\"metadata\":{},\"name\":\"GE65_Sprsn_Flag\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"GE65_Tot_Clms\",\"nullable\":true,\"type\":\"integer\"},{\"metadata\":{},\"name\":\"GE65_Tot_30day_Fills\",\"nullable\":true,\"type\":\"integer\"},{\"metadata\":{},\"name\":\"GE65_Tot_Drug_Cst\",\"nullable\":true,\"type\":\"float\"},{\"metadata\":{},\"name\":\"GE65_Tot_Day_Suply\",\"nullable\":true,\"type\":\"integer\"},{\"metadata\":{},\"name\":\"GE65_Bene_Sprsn_Flag\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"GE65_Tot_Benes\",\"nullable\":true,\"type\":\"integer\"},{\"metadata\":{},\"name\":\"filename\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"year\",\"nullable\":true,\"type\":\"string\"}],\"type\":\"struct\"}'\n","\n","schematable = StructType.fromJson(json.loads(jschematable))"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"f423086e-808a-4b6a-ae13-87c4e1daa3ae","statement_id":4,"state":"finished","livy_statement_state":"available","queued_time":"2023-09-13T15:46:00.1331402Z","session_start_time":null,"execution_start_time":"2023-09-13T15:48:55.1673625Z","execution_finish_time":"2023-09-13T15:48:55.4852985Z","spark_jobs":{"numbers":{"SUCCEEDED":0,"UNKNOWN":0,"FAILED":0,"RUNNING":0},"jobs":[],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"c7b6b69b-de85-41e1-b79c-7bc18cd18a5b"},"text/plain":"StatementMeta(, f423086e-808a-4b6a-ae13-87c4e1daa3ae, 4, Finished, Available)"},"metadata":{}}],"execution_count":2,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"e507dba2-3ff4-4fab-af3a-5d3535b4d8a0"},{"cell_type":"code","source":["from pyspark.sql.types import *\n","from  pyspark.sql.functions import *\n","\n","def loadFullDataFromSource(file_name):\n","    sqljoin= \"SELECT 1 as joinyear1, right(substring_index('\" + file_name + \"', '.', 1),4) as fourdigityear\"\n","    dfyear = spark.sql(sqljoin)\n","    df = spark.read.format(\"csv\").option(\"header\",\"true\").schema(schemacsv).load(\"Files/medicare/demorawfiles/\" + file_name)\n","    df = df.withColumn(\"filename\", input_file_name())\n","    df = df.withColumn(\"joinyear2\", lit(1))\n","    df = df.join(dfyear, df.joinyear2 == dfyear.joinyear1, 'inner')\n","    df = df.drop(\"joinyear1\",\"joinyear2\")\n","    #display(df) \n","    df.write.mode(\"append\").format(\"delta\").option(\"mergeSchema\", \"true\").option(\"schema\", \"schematable\").save(\"Tables/medicarepartd_demo3\")\n","    \n","full_files = [\n","\n","    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2013.csv',\n","    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2014.csv',\n","    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2015.csv',\n","    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2016.csv',\n","    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2017.csv',\n","    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_2018.csv',\n","    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_2019.csv',\n","    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_2020.csv',\n","    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_2021.csv'\n","    ]\n","\n","for file in full_files:\n","    loadFullDataFromSource(file)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"f423086e-808a-4b6a-ae13-87c4e1daa3ae","statement_id":5,"state":"finished","livy_statement_state":"available","queued_time":"2023-09-13T15:46:00.1337308Z","session_start_time":null,"execution_start_time":"2023-09-13T15:48:56.0267489Z","execution_finish_time":"2023-09-13T16:10:27.2322776Z","spark_jobs":{"numbers":{"SUCCEEDED":45,"UNKNOWN":0,"FAILED":0,"RUNNING":0},"jobs":[{"displayName":"toString at String.java:2994","dataWritten":0,"dataRead":4729,"rowCount":50,"usageDescription":"","jobId":61,"name":"toString at String.java:2994","description":"Delta: Job group for statement 5:\nfrom pyspark.sql.types import *\nfrom  pyspark.sql.functions import *\n\ndef loadFullDataFromSource(file_name):\n    sqljoin= \"SELECT 1 as joinyear1, right(substring_index('\" + file_name + \"', '.', 1),4) as fourdigityear\"\n    dfyear = spark.sql(sqljoin)\n    df = spark.read.format(\"csv\").option(\"header\",\"true\").schema(schemacsv).load(\"Files/medicare/demorawfiles/\" + file_name)\n    df = df.withColumn(\"filename\", input_file_name())\n    df = df.withColumn(\"joinyear2\", lit(1))\n    df = df.join(dfyear, df.joinyear2 == dfyear.joinyear1, 'inner')\n    df = df.drop(\"joinyear1\",\"joinyear2\")\n    #display(df) \n    df.write.mode(\"append\").format(\"delta\").option(\"mergeSchema\", \"true\").option(\"schema\", \"schematable\").save(\"Tables/medicarepartd_demo2\")\n    \nfull_files = [\n\n    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2013.csv',\n    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2014.csv',\n    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2015.csv',\n    'Medicare_Part_D_Prescribers_by_Provid...: Compute snapshot for version: 8","submissionTime":"2023-09-13T16:10:26.529GMT","completionTime":"2023-09-13T16:10:26.554GMT","stageIds":[99,100,101],"jobGroup":"5","status":"SUCCEEDED","numTasks":60,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":59,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":4729,"dataRead":19074,"rowCount":70,"usageDescription":"","jobId":60,"name":"toString at String.java:2994","description":"Delta: Job group for statement 5:\nfrom pyspark.sql.types import *\nfrom  pyspark.sql.functions import *\n\ndef loadFullDataFromSource(file_name):\n    sqljoin= \"SELECT 1 as joinyear1, right(substring_index('\" + file_name + \"', '.', 1),4) as fourdigityear\"\n    dfyear = spark.sql(sqljoin)\n    df = spark.read.format(\"csv\").option(\"header\",\"true\").schema(schemacsv).load(\"Files/medicare/demorawfiles/\" + file_name)\n    df = df.withColumn(\"filename\", input_file_name())\n    df = df.withColumn(\"joinyear2\", lit(1))\n    df = df.join(dfyear, df.joinyear2 == dfyear.joinyear1, 'inner')\n    df = df.drop(\"joinyear1\",\"joinyear2\")\n    #display(df) \n    df.write.mode(\"append\").format(\"delta\").option(\"mergeSchema\", \"true\").option(\"schema\", \"schematable\").save(\"Tables/medicarepartd_demo2\")\n    \nfull_files = [\n\n    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2013.csv',\n    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2014.csv',\n    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2015.csv',\n    'Medicare_Part_D_Prescribers_by_Provid...: Compute snapshot for version: 8","submissionTime":"2023-09-13T16:10:26.279GMT","completionTime":"2023-09-13T16:10:26.509GMT","stageIds":[97,98],"jobGroup":"5","status":"SUCCEEDED","numTasks":59,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":9,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":19074,"dataRead":26548,"rowCount":40,"usageDescription":"","jobId":59,"name":"toString at String.java:2994","description":"Delta: Job group for statement 5:\nfrom pyspark.sql.types import *\nfrom  pyspark.sql.functions import *\n\ndef loadFullDataFromSource(file_name):\n    sqljoin= \"SELECT 1 as joinyear1, right(substring_index('\" + file_name + \"', '.', 1),4) as fourdigityear\"\n    dfyear = spark.sql(sqljoin)\n    df = spark.read.format(\"csv\").option(\"header\",\"true\").schema(schemacsv).load(\"Files/medicare/demorawfiles/\" + file_name)\n    df = df.withColumn(\"filename\", input_file_name())\n    df = df.withColumn(\"joinyear2\", lit(1))\n    df = df.join(dfyear, df.joinyear2 == dfyear.joinyear1, 'inner')\n    df = df.drop(\"joinyear1\",\"joinyear2\")\n    #display(df) \n    df.write.mode(\"append\").format(\"delta\").option(\"mergeSchema\", \"true\").option(\"schema\", \"schematable\").save(\"Tables/medicarepartd_demo2\")\n    \nfull_files = [\n\n    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2013.csv',\n    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2014.csv',\n    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2015.csv',\n    'Medicare_Part_D_Prescribers_by_Provid...: Compute snapshot for version: 8","submissionTime":"2023-09-13T16:10:26.070GMT","completionTime":"2023-09-13T16:10:26.165GMT","stageIds":[96],"jobGroup":"5","status":"SUCCEEDED","numTasks":9,"numActiveTasks":0,"numCompletedTasks":9,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":9,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"save at NativeMethodAccessorImpl.java:0","dataWritten":514673745,"dataRead":1766088795,"rowCount":50463724,"usageDescription":"","jobId":58,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 5:\nfrom pyspark.sql.types import *\nfrom  pyspark.sql.functions import *\n\ndef loadFullDataFromSource(file_name):\n    sqljoin= \"SELECT 1 as joinyear1, right(substring_index('\" + file_name + \"', '.', 1),4) as fourdigityear\"\n    dfyear = spark.sql(sqljoin)\n    df = spark.read.format(\"csv\").option(\"header\",\"true\").schema(schemacsv).load(\"Files/medicare/demorawfiles/\" + file_name)\n    df = df.withColumn(\"filename\", input_file_name())\n    df = df.withColumn(\"joinyear2\", lit(1))\n    df = df.join(dfyear, df.joinyear2 == dfyear.joinyear1, 'inner')\n    df = df.drop(\"joinyear1\",\"joinyear2\")\n    #display(df) \n    df.write.mode(\"append\").format(\"delta\").option(\"mergeSchema\", \"true\").option(\"schema\", \"schematable\").save(\"Tables/medicarepartd_demo2\")\n    \nfull_files = [\n\n    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2013.csv',\n    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2014.csv',\n    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2015.csv',\n    'Medicare_Part_D_Prescribers_by_Provid...","submissionTime":"2023-09-13T16:08:18.329GMT","completionTime":"2023-09-13T16:10:25.623GMT","stageIds":[94,95],"jobGroup":"5","status":"SUCCEEDED","numTasks":28,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":27,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"save at NativeMethodAccessorImpl.java:0","dataWritten":1766088795,"dataRead":3624738993,"rowCount":50463724,"usageDescription":"","jobId":57,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 5:\nfrom pyspark.sql.types import *\nfrom  pyspark.sql.functions import *\n\ndef loadFullDataFromSource(file_name):\n    sqljoin= \"SELECT 1 as joinyear1, right(substring_index('\" + file_name + \"', '.', 1),4) as fourdigityear\"\n    dfyear = spark.sql(sqljoin)\n    df = spark.read.format(\"csv\").option(\"header\",\"true\").schema(schemacsv).load(\"Files/medicare/demorawfiles/\" + file_name)\n    df = df.withColumn(\"filename\", input_file_name())\n    df = df.withColumn(\"joinyear2\", lit(1))\n    df = df.join(dfyear, df.joinyear2 == dfyear.joinyear1, 'inner')\n    df = df.drop(\"joinyear1\",\"joinyear2\")\n    #display(df) \n    df.write.mode(\"append\").format(\"delta\").option(\"mergeSchema\", \"true\").option(\"schema\", \"schematable\").save(\"Tables/medicarepartd_demo2\")\n    \nfull_files = [\n\n    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2013.csv',\n    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2014.csv',\n    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2015.csv',\n    'Medicare_Part_D_Prescribers_by_Provid...","submissionTime":"2023-09-13T16:07:56.067GMT","completionTime":"2023-09-13T16:08:18.280GMT","stageIds":[93],"jobGroup":"5","status":"SUCCEEDED","numTasks":27,"numActiveTasks":0,"numCompletedTasks":27,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":27,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":0,"dataRead":4714,"rowCount":50,"usageDescription":"","jobId":55,"name":"toString at String.java:2994","description":"Delta: Job group for statement 5:\nfrom pyspark.sql.types import *\nfrom  pyspark.sql.functions import *\n\ndef loadFullDataFromSource(file_name):\n    sqljoin= \"SELECT 1 as joinyear1, right(substring_index('\" + file_name + \"', '.', 1),4) as fourdigityear\"\n    dfyear = spark.sql(sqljoin)\n    df = spark.read.format(\"csv\").option(\"header\",\"true\").schema(schemacsv).load(\"Files/medicare/demorawfiles/\" + file_name)\n    df = df.withColumn(\"filename\", input_file_name())\n    df = df.withColumn(\"joinyear2\", lit(1))\n    df = df.join(dfyear, df.joinyear2 == dfyear.joinyear1, 'inner')\n    df = df.drop(\"joinyear1\",\"joinyear2\")\n    #display(df) \n    df.write.mode(\"append\").format(\"delta\").option(\"mergeSchema\", \"true\").option(\"schema\", \"schematable\").save(\"Tables/medicarepartd_demo2\")\n    \nfull_files = [\n\n    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2013.csv',\n    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2014.csv',\n    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2015.csv',\n    'Medicare_Part_D_Prescribers_by_Provid...: Compute snapshot for version: 7","submissionTime":"2023-09-13T16:07:55.778GMT","completionTime":"2023-09-13T16:07:55.801GMT","stageIds":[89,90,91],"jobGroup":"5","status":"SUCCEEDED","numTasks":59,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":58,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":4714,"dataRead":16999,"rowCount":68,"usageDescription":"","jobId":54,"name":"toString at String.java:2994","description":"Delta: Job group for statement 5:\nfrom pyspark.sql.types import *\nfrom  pyspark.sql.functions import *\n\ndef loadFullDataFromSource(file_name):\n    sqljoin= \"SELECT 1 as joinyear1, right(substring_index('\" + file_name + \"', '.', 1),4) as fourdigityear\"\n    dfyear = spark.sql(sqljoin)\n    df = spark.read.format(\"csv\").option(\"header\",\"true\").schema(schemacsv).load(\"Files/medicare/demorawfiles/\" + file_name)\n    df = df.withColumn(\"filename\", input_file_name())\n    df = df.withColumn(\"joinyear2\", lit(1))\n    df = df.join(dfyear, df.joinyear2 == dfyear.joinyear1, 'inner')\n    df = df.drop(\"joinyear1\",\"joinyear2\")\n    #display(df) \n    df.write.mode(\"append\").format(\"delta\").option(\"mergeSchema\", \"true\").option(\"schema\", \"schematable\").save(\"Tables/medicarepartd_demo2\")\n    \nfull_files = [\n\n    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2013.csv',\n    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2014.csv',\n    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2015.csv',\n    'Medicare_Part_D_Prescribers_by_Provid...: Compute snapshot for version: 7","submissionTime":"2023-09-13T16:07:55.514GMT","completionTime":"2023-09-13T16:07:55.765GMT","stageIds":[88,87],"jobGroup":"5","status":"SUCCEEDED","numTasks":58,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":8,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":16999,"dataRead":23839,"rowCount":36,"usageDescription":"","jobId":53,"name":"toString at String.java:2994","description":"Delta: Job group for statement 5:\nfrom pyspark.sql.types import *\nfrom  pyspark.sql.functions import *\n\ndef loadFullDataFromSource(file_name):\n    sqljoin= \"SELECT 1 as joinyear1, right(substring_index('\" + file_name + \"', '.', 1),4) as fourdigityear\"\n    dfyear = spark.sql(sqljoin)\n    df = spark.read.format(\"csv\").option(\"header\",\"true\").schema(schemacsv).load(\"Files/medicare/demorawfiles/\" + file_name)\n    df = df.withColumn(\"filename\", input_file_name())\n    df = df.withColumn(\"joinyear2\", lit(1))\n    df = df.join(dfyear, df.joinyear2 == dfyear.joinyear1, 'inner')\n    df = df.drop(\"joinyear1\",\"joinyear2\")\n    #display(df) \n    df.write.mode(\"append\").format(\"delta\").option(\"mergeSchema\", \"true\").option(\"schema\", \"schematable\").save(\"Tables/medicarepartd_demo2\")\n    \nfull_files = [\n\n    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2013.csv',\n    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2014.csv',\n    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2015.csv',\n    'Medicare_Part_D_Prescribers_by_Provid...: Compute snapshot for version: 7","submissionTime":"2023-09-13T16:07:55.248GMT","completionTime":"2023-09-13T16:07:55.349GMT","stageIds":[86],"jobGroup":"5","status":"SUCCEEDED","numTasks":8,"numActiveTasks":0,"numCompletedTasks":8,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":8,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"save at NativeMethodAccessorImpl.java:0","dataWritten":512893122,"dataRead":1763065756,"rowCount":50419458,"usageDescription":"","jobId":52,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 5:\nfrom pyspark.sql.types import *\nfrom  pyspark.sql.functions import *\n\ndef loadFullDataFromSource(file_name):\n    sqljoin= \"SELECT 1 as joinyear1, right(substring_index('\" + file_name + \"', '.', 1),4) as fourdigityear\"\n    dfyear = spark.sql(sqljoin)\n    df = spark.read.format(\"csv\").option(\"header\",\"true\").schema(schemacsv).load(\"Files/medicare/demorawfiles/\" + file_name)\n    df = df.withColumn(\"filename\", input_file_name())\n    df = df.withColumn(\"joinyear2\", lit(1))\n    df = df.join(dfyear, df.joinyear2 == dfyear.joinyear1, 'inner')\n    df = df.drop(\"joinyear1\",\"joinyear2\")\n    #display(df) \n    df.write.mode(\"append\").format(\"delta\").option(\"mergeSchema\", \"true\").option(\"schema\", \"schematable\").save(\"Tables/medicarepartd_demo2\")\n    \nfull_files = [\n\n    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2013.csv',\n    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2014.csv',\n    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2015.csv',\n    'Medicare_Part_D_Prescribers_by_Provid...","submissionTime":"2023-09-13T16:05:51.947GMT","completionTime":"2023-09-13T16:07:54.800GMT","stageIds":[84,85],"jobGroup":"5","status":"SUCCEEDED","numTasks":28,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":27,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"save at NativeMethodAccessorImpl.java:0","dataWritten":1763065756,"dataRead":3619495437,"rowCount":50419458,"usageDescription":"","jobId":51,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 5:\nfrom pyspark.sql.types import *\nfrom  pyspark.sql.functions import *\n\ndef loadFullDataFromSource(file_name):\n    sqljoin= \"SELECT 1 as joinyear1, right(substring_index('\" + file_name + \"', '.', 1),4) as fourdigityear\"\n    dfyear = spark.sql(sqljoin)\n    df = spark.read.format(\"csv\").option(\"header\",\"true\").schema(schemacsv).load(\"Files/medicare/demorawfiles/\" + file_name)\n    df = df.withColumn(\"filename\", input_file_name())\n    df = df.withColumn(\"joinyear2\", lit(1))\n    df = df.join(dfyear, df.joinyear2 == dfyear.joinyear1, 'inner')\n    df = df.drop(\"joinyear1\",\"joinyear2\")\n    #display(df) \n    df.write.mode(\"append\").format(\"delta\").option(\"mergeSchema\", \"true\").option(\"schema\", \"schematable\").save(\"Tables/medicarepartd_demo2\")\n    \nfull_files = [\n\n    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2013.csv',\n    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2014.csv',\n    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2015.csv',\n    'Medicare_Part_D_Prescribers_by_Provid...","submissionTime":"2023-09-13T16:05:29.208GMT","completionTime":"2023-09-13T16:05:51.901GMT","stageIds":[83],"jobGroup":"5","status":"SUCCEEDED","numTasks":27,"numActiveTasks":0,"numCompletedTasks":27,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":27,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":0,"dataRead":4699,"rowCount":50,"usageDescription":"","jobId":49,"name":"toString at String.java:2994","description":"Delta: Job group for statement 5:\nfrom pyspark.sql.types import *\nfrom  pyspark.sql.functions import *\n\ndef loadFullDataFromSource(file_name):\n    sqljoin= \"SELECT 1 as joinyear1, right(substring_index('\" + file_name + \"', '.', 1),4) as fourdigityear\"\n    dfyear = spark.sql(sqljoin)\n    df = spark.read.format(\"csv\").option(\"header\",\"true\").schema(schemacsv).load(\"Files/medicare/demorawfiles/\" + file_name)\n    df = df.withColumn(\"filename\", input_file_name())\n    df = df.withColumn(\"joinyear2\", lit(1))\n    df = df.join(dfyear, df.joinyear2 == dfyear.joinyear1, 'inner')\n    df = df.drop(\"joinyear1\",\"joinyear2\")\n    #display(df) \n    df.write.mode(\"append\").format(\"delta\").option(\"mergeSchema\", \"true\").option(\"schema\", \"schematable\").save(\"Tables/medicarepartd_demo2\")\n    \nfull_files = [\n\n    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2013.csv',\n    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2014.csv',\n    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2015.csv',\n    'Medicare_Part_D_Prescribers_by_Provid...: Compute snapshot for version: 6","submissionTime":"2023-09-13T16:05:28.904GMT","completionTime":"2023-09-13T16:05:28.928GMT","stageIds":[81,79,80],"jobGroup":"5","status":"SUCCEEDED","numTasks":58,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":57,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":4699,"dataRead":14955,"rowCount":66,"usageDescription":"","jobId":48,"name":"toString at String.java:2994","description":"Delta: Job group for statement 5:\nfrom pyspark.sql.types import *\nfrom  pyspark.sql.functions import *\n\ndef loadFullDataFromSource(file_name):\n    sqljoin= \"SELECT 1 as joinyear1, right(substring_index('\" + file_name + \"', '.', 1),4) as fourdigityear\"\n    dfyear = spark.sql(sqljoin)\n    df = spark.read.format(\"csv\").option(\"header\",\"true\").schema(schemacsv).load(\"Files/medicare/demorawfiles/\" + file_name)\n    df = df.withColumn(\"filename\", input_file_name())\n    df = df.withColumn(\"joinyear2\", lit(1))\n    df = df.join(dfyear, df.joinyear2 == dfyear.joinyear1, 'inner')\n    df = df.drop(\"joinyear1\",\"joinyear2\")\n    #display(df) \n    df.write.mode(\"append\").format(\"delta\").option(\"mergeSchema\", \"true\").option(\"schema\", \"schematable\").save(\"Tables/medicarepartd_demo2\")\n    \nfull_files = [\n\n    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2013.csv',\n    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2014.csv',\n    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2015.csv',\n    'Medicare_Part_D_Prescribers_by_Provid...: Compute snapshot for version: 6","submissionTime":"2023-09-13T16:05:28.615GMT","completionTime":"2023-09-13T16:05:28.887GMT","stageIds":[78,77],"jobGroup":"5","status":"SUCCEEDED","numTasks":57,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":7,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":14955,"dataRead":21145,"rowCount":32,"usageDescription":"","jobId":47,"name":"toString at String.java:2994","description":"Delta: Job group for statement 5:\nfrom pyspark.sql.types import *\nfrom  pyspark.sql.functions import *\n\ndef loadFullDataFromSource(file_name):\n    sqljoin= \"SELECT 1 as joinyear1, right(substring_index('\" + file_name + \"', '.', 1),4) as fourdigityear\"\n    dfyear = spark.sql(sqljoin)\n    df = spark.read.format(\"csv\").option(\"header\",\"true\").schema(schemacsv).load(\"Files/medicare/demorawfiles/\" + file_name)\n    df = df.withColumn(\"filename\", input_file_name())\n    df = df.withColumn(\"joinyear2\", lit(1))\n    df = df.join(dfyear, df.joinyear2 == dfyear.joinyear1, 'inner')\n    df = df.drop(\"joinyear1\",\"joinyear2\")\n    #display(df) \n    df.write.mode(\"append\").format(\"delta\").option(\"mergeSchema\", \"true\").option(\"schema\", \"schematable\").save(\"Tables/medicarepartd_demo2\")\n    \nfull_files = [\n\n    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2013.csv',\n    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2014.csv',\n    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2015.csv',\n    'Medicare_Part_D_Prescribers_by_Provid...: Compute snapshot for version: 6","submissionTime":"2023-09-13T16:05:28.383GMT","completionTime":"2023-09-13T16:05:28.501GMT","stageIds":[76],"jobGroup":"5","status":"SUCCEEDED","numTasks":7,"numActiveTasks":0,"numCompletedTasks":7,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":7,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"save at NativeMethodAccessorImpl.java:0","dataWritten":516065442,"dataRead":1778389570,"rowCount":50803740,"usageDescription":"","jobId":46,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 5:\nfrom pyspark.sql.types import *\nfrom  pyspark.sql.functions import *\n\ndef loadFullDataFromSource(file_name):\n    sqljoin= \"SELECT 1 as joinyear1, right(substring_index('\" + file_name + \"', '.', 1),4) as fourdigityear\"\n    dfyear = spark.sql(sqljoin)\n    df = spark.read.format(\"csv\").option(\"header\",\"true\").schema(schemacsv).load(\"Files/medicare/demorawfiles/\" + file_name)\n    df = df.withColumn(\"filename\", input_file_name())\n    df = df.withColumn(\"joinyear2\", lit(1))\n    df = df.join(dfyear, df.joinyear2 == dfyear.joinyear1, 'inner')\n    df = df.drop(\"joinyear1\",\"joinyear2\")\n    #display(df) \n    df.write.mode(\"append\").format(\"delta\").option(\"mergeSchema\", \"true\").option(\"schema\", \"schematable\").save(\"Tables/medicarepartd_demo2\")\n    \nfull_files = [\n\n    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2013.csv',\n    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2014.csv',\n    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2015.csv',\n    'Medicare_Part_D_Prescribers_by_Provid...","submissionTime":"2023-09-13T16:03:22.061GMT","completionTime":"2023-09-13T16:05:27.937GMT","stageIds":[74,75],"jobGroup":"5","status":"SUCCEEDED","numTasks":29,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":28,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"save at NativeMethodAccessorImpl.java:0","dataWritten":1778389570,"dataRead":3640459324,"rowCount":50803740,"usageDescription":"","jobId":45,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 5:\nfrom pyspark.sql.types import *\nfrom  pyspark.sql.functions import *\n\ndef loadFullDataFromSource(file_name):\n    sqljoin= \"SELECT 1 as joinyear1, right(substring_index('\" + file_name + \"', '.', 1),4) as fourdigityear\"\n    dfyear = spark.sql(sqljoin)\n    df = spark.read.format(\"csv\").option(\"header\",\"true\").schema(schemacsv).load(\"Files/medicare/demorawfiles/\" + file_name)\n    df = df.withColumn(\"filename\", input_file_name())\n    df = df.withColumn(\"joinyear2\", lit(1))\n    df = df.join(dfyear, df.joinyear2 == dfyear.joinyear1, 'inner')\n    df = df.drop(\"joinyear1\",\"joinyear2\")\n    #display(df) \n    df.write.mode(\"append\").format(\"delta\").option(\"mergeSchema\", \"true\").option(\"schema\", \"schematable\").save(\"Tables/medicarepartd_demo2\")\n    \nfull_files = [\n\n    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2013.csv',\n    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2014.csv',\n    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2015.csv',\n    'Medicare_Part_D_Prescribers_by_Provid...","submissionTime":"2023-09-13T16:02:59.939GMT","completionTime":"2023-09-13T16:03:22.016GMT","stageIds":[73],"jobGroup":"5","status":"SUCCEEDED","numTasks":28,"numActiveTasks":0,"numCompletedTasks":28,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":28,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":0,"dataRead":4684,"rowCount":50,"usageDescription":"","jobId":43,"name":"toString at String.java:2994","description":"Delta: Job group for statement 5:\nfrom pyspark.sql.types import *\nfrom  pyspark.sql.functions import *\n\ndef loadFullDataFromSource(file_name):\n    sqljoin= \"SELECT 1 as joinyear1, right(substring_index('\" + file_name + \"', '.', 1),4) as fourdigityear\"\n    dfyear = spark.sql(sqljoin)\n    df = spark.read.format(\"csv\").option(\"header\",\"true\").schema(schemacsv).load(\"Files/medicare/demorawfiles/\" + file_name)\n    df = df.withColumn(\"filename\", input_file_name())\n    df = df.withColumn(\"joinyear2\", lit(1))\n    df = df.join(dfyear, df.joinyear2 == dfyear.joinyear1, 'inner')\n    df = df.drop(\"joinyear1\",\"joinyear2\")\n    #display(df) \n    df.write.mode(\"append\").format(\"delta\").option(\"mergeSchema\", \"true\").option(\"schema\", \"schematable\").save(\"Tables/medicarepartd_demo2\")\n    \nfull_files = [\n\n    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2013.csv',\n    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2014.csv',\n    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2015.csv',\n    'Medicare_Part_D_Prescribers_by_Provid...: Compute snapshot for version: 5","submissionTime":"2023-09-13T16:02:59.644GMT","completionTime":"2023-09-13T16:02:59.676GMT","stageIds":[70,71,69],"jobGroup":"5","status":"SUCCEEDED","numTasks":57,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":56,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":4684,"dataRead":12871,"rowCount":64,"usageDescription":"","jobId":42,"name":"toString at String.java:2994","description":"Delta: Job group for statement 5:\nfrom pyspark.sql.types import *\nfrom  pyspark.sql.functions import *\n\ndef loadFullDataFromSource(file_name):\n    sqljoin= \"SELECT 1 as joinyear1, right(substring_index('\" + file_name + \"', '.', 1),4) as fourdigityear\"\n    dfyear = spark.sql(sqljoin)\n    df = spark.read.format(\"csv\").option(\"header\",\"true\").schema(schemacsv).load(\"Files/medicare/demorawfiles/\" + file_name)\n    df = df.withColumn(\"filename\", input_file_name())\n    df = df.withColumn(\"joinyear2\", lit(1))\n    df = df.join(dfyear, df.joinyear2 == dfyear.joinyear1, 'inner')\n    df = df.drop(\"joinyear1\",\"joinyear2\")\n    #display(df) \n    df.write.mode(\"append\").format(\"delta\").option(\"mergeSchema\", \"true\").option(\"schema\", \"schematable\").save(\"Tables/medicarepartd_demo2\")\n    \nfull_files = [\n\n    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2013.csv',\n    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2014.csv',\n    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2015.csv',\n    'Medicare_Part_D_Prescribers_by_Provid...: Compute snapshot for version: 5","submissionTime":"2023-09-13T16:02:59.364GMT","completionTime":"2023-09-13T16:02:59.594GMT","stageIds":[67,68],"jobGroup":"5","status":"SUCCEEDED","numTasks":56,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":6,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":12871,"dataRead":18437,"rowCount":28,"usageDescription":"","jobId":41,"name":"toString at String.java:2994","description":"Delta: Job group for statement 5:\nfrom pyspark.sql.types import *\nfrom  pyspark.sql.functions import *\n\ndef loadFullDataFromSource(file_name):\n    sqljoin= \"SELECT 1 as joinyear1, right(substring_index('\" + file_name + \"', '.', 1),4) as fourdigityear\"\n    dfyear = spark.sql(sqljoin)\n    df = spark.read.format(\"csv\").option(\"header\",\"true\").schema(schemacsv).load(\"Files/medicare/demorawfiles/\" + file_name)\n    df = df.withColumn(\"filename\", input_file_name())\n    df = df.withColumn(\"joinyear2\", lit(1))\n    df = df.join(dfyear, df.joinyear2 == dfyear.joinyear1, 'inner')\n    df = df.drop(\"joinyear1\",\"joinyear2\")\n    #display(df) \n    df.write.mode(\"append\").format(\"delta\").option(\"mergeSchema\", \"true\").option(\"schema\", \"schematable\").save(\"Tables/medicarepartd_demo2\")\n    \nfull_files = [\n\n    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2013.csv',\n    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2014.csv',\n    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2015.csv',\n    'Medicare_Part_D_Prescribers_by_Provid...: Compute snapshot for version: 5","submissionTime":"2023-09-13T16:02:59.163GMT","completionTime":"2023-09-13T16:02:59.258GMT","stageIds":[66],"jobGroup":"5","status":"SUCCEEDED","numTasks":6,"numActiveTasks":0,"numCompletedTasks":6,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":6,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"save at NativeMethodAccessorImpl.java:0","dataWritten":511298045,"dataRead":1772559087,"rowCount":50623200,"usageDescription":"","jobId":40,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 5:\nfrom pyspark.sql.types import *\nfrom  pyspark.sql.functions import *\n\ndef loadFullDataFromSource(file_name):\n    sqljoin= \"SELECT 1 as joinyear1, right(substring_index('\" + file_name + \"', '.', 1),4) as fourdigityear\"\n    dfyear = spark.sql(sqljoin)\n    df = spark.read.format(\"csv\").option(\"header\",\"true\").schema(schemacsv).load(\"Files/medicare/demorawfiles/\" + file_name)\n    df = df.withColumn(\"filename\", input_file_name())\n    df = df.withColumn(\"joinyear2\", lit(1))\n    df = df.join(dfyear, df.joinyear2 == dfyear.joinyear1, 'inner')\n    df = df.drop(\"joinyear1\",\"joinyear2\")\n    #display(df) \n    df.write.mode(\"append\").format(\"delta\").option(\"mergeSchema\", \"true\").option(\"schema\", \"schematable\").save(\"Tables/medicarepartd_demo2\")\n    \nfull_files = [\n\n    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2013.csv',\n    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2014.csv',\n    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2015.csv',\n    'Medicare_Part_D_Prescribers_by_Provid...","submissionTime":"2023-09-13T16:00:51.728GMT","completionTime":"2023-09-13T16:02:58.738GMT","stageIds":[64,65],"jobGroup":"5","status":"SUCCEEDED","numTasks":28,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":27,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"save at NativeMethodAccessorImpl.java:0","dataWritten":1772559087,"dataRead":3623598831,"rowCount":50623200,"usageDescription":"","jobId":39,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 5:\nfrom pyspark.sql.types import *\nfrom  pyspark.sql.functions import *\n\ndef loadFullDataFromSource(file_name):\n    sqljoin= \"SELECT 1 as joinyear1, right(substring_index('\" + file_name + \"', '.', 1),4) as fourdigityear\"\n    dfyear = spark.sql(sqljoin)\n    df = spark.read.format(\"csv\").option(\"header\",\"true\").schema(schemacsv).load(\"Files/medicare/demorawfiles/\" + file_name)\n    df = df.withColumn(\"filename\", input_file_name())\n    df = df.withColumn(\"joinyear2\", lit(1))\n    df = df.join(dfyear, df.joinyear2 == dfyear.joinyear1, 'inner')\n    df = df.drop(\"joinyear1\",\"joinyear2\")\n    #display(df) \n    df.write.mode(\"append\").format(\"delta\").option(\"mergeSchema\", \"true\").option(\"schema\", \"schematable\").save(\"Tables/medicarepartd_demo2\")\n    \nfull_files = [\n\n    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2013.csv',\n    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2014.csv',\n    'Medicare_Part_D_Prescribers_by_Provider_and_Drug_Dataset_2015.csv',\n    'Medicare_Part_D_Prescribers_by_Provid...","submissionTime":"2023-09-13T16:00:30.231GMT","completionTime":"2023-09-13T16:00:51.676GMT","stageIds":[63],"jobGroup":"5","status":"SUCCEEDED","numTasks":27,"numActiveTasks":0,"numCompletedTasks":27,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":27,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"b53fb517-5b87-4774-bfcd-aa55dee1f5ef"},"text/plain":"StatementMeta(, f423086e-808a-4b6a-ae13-87c4e1daa3ae, 5, Finished, Available)"},"metadata":{}}],"execution_count":3,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"advisor":{"adviceMetadata":"{\"artifactId\":\"9e91bf48-dc09-425f-bb81-db39a923a85c\",\"activityId\":\"f423086e-808a-4b6a-ae13-87c4e1daa3ae\",\"applicationId\":\"application_1694620043924_0001\",\"jobGroupId\":\"5\",\"advices\":{\"info\":9}}"}},"id":"ee67a93c-f293-4996-a9fd-edcdcc717c6a"}],"metadata":{"language_info":{"name":"python"},"kernelspec":{"name":"synapse_pyspark","display_name":"Synapse PySpark"},"microsoft":{"host":{"synapse_widget":{"token":"f64b5c25-b19f-48b3-ad55-fd2c46f301e7","state":{"66515eeb-bab1-4fc7-8452-c677ee4f6d59":{"type":"Synapse.DataFrame","sync_state":{"table":{"rows":[{"0":"1","1":"2018","index":1}],"schema":[{"key":"0","name":"joinyear","type":"int"},{"key":"1","name":"fourdigityear","type":"string"}],"truncated":false},"isSummary":false,"language":"scala"},"persist_state":{"view":{"type":"details","tableOptions":{},"chartOptions":{"chartType":"bar","categoryFieldKeys":["1"],"seriesFieldKeys":["0"],"aggregationType":"sum","isStacked":false,"binsNumber":10,"wordFrequency":"-1"}}}}}}},"language":"python","ms_spell_check":{"ms_spell_check_language":"en"}},"widgets":{},"kernel_info":{"name":"synapse_pyspark"},"nteract":{"version":"nteract-front-end@1.0.0"},"save_output":true,"spark_compute":{"compute_id":"/trident/default","session_options":{"enableDebugMode":false,"conf":{}}},"notebook_environment":{},"synapse_widget":{"version":"0.1","state":{}},"trident":{"lakehouse":{"default_lakehouse":"2a021a4f-3660-4f82-91d8-49bbf7dec376","known_lakehouses":[{"id":"2a021a4f-3660-4f82-91d8-49bbf7dec376"}]}}},"nbformat":4,"nbformat_minor":5}